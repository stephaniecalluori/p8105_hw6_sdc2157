---
title: "p8105_hw6_sdc2157"
author: "Stephanie Calluori"
date: 2023-12-02
output: github_document
---

# Load packages and set seed
```{r packages, message = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(readr)

set.seed(1)

```

```{r setup, message = FALSE, echo = FALSE, results = FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
## Load and clean data
```{r, load and clean homicide data, message = FALSE}
homicide_raw <- read_csv("data/homicide-data.csv", col_names = TRUE, na = c("", "NA", "Unknown"))

homicide_clean <- homicide_raw |> 
  janitor::clean_names() |> 
  mutate(victim_age = as.numeric(victim_age)) |> 
  mutate(city_state = paste(city, state, sep = ", ")) |> 
  filter(!city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO")) |> 
  filter(victim_race == "White" | victim_race == "Black") |> 
  mutate(
    victim_race = fct_relevel(victim_race, "White"),
    victim_sex = fct_relevel(victim_sex, "Female"),
    resolved = as.numeric(disposition == "Closed by arrest"))

```

The Washington Post gathered data on homicides in 50 large US cities to examine the number and characteristics of homicides that go solved vs unsolved. The variables in the dataset include city and state name, details about the victim (race, age, sex), and case status. 

Note: We created a `resolved` variable where Closed by arrest = 1, Closed without arrest = 0, and Open/No arrest = 0.

## Construct a logistic regression

We constructed a logistic regression with our resolved variable as the outcome and predictors of victim age, sex, and race.

```{r, baltimore analysis}
baltimore_df <- homicide_clean |> 
  filter(city_state == "Baltimore, MD")

fit_baltimore <- glm(resolved ~ victim_age + victim_sex + victim_race, data = baltimore_df, family = binomial())

fit_baltimore |> 
  broom::tidy(conf.int = TRUE) |> 
  rename(log_OR = estimate, log_conf_low = conf.low, log_conf_high = conf.high) |> 
  mutate(OR = exp(log_OR),
         conf_low = exp(log_conf_low),
         conf_high = exp(log_conf_high)) |> 
  select(term, OR, conf_low, conf_high, p.value) |> 
  knitr::kable(digits = 3)
  
```

In Baltimore, the odds of solving a case in which the victim is male are 0.426 times lower than the odds of solving a case in which the victim is female. 95% CI (0.324, 0.558)


```{r, cities analysis}
homicide_nest_df <- homicide_clean |> 
  select(city_state, everything()) |> 
  nest(.data = _, data = uid:resolved)

results <- homicide_nest_df |> 
  mutate(
    models = map(data, \(df) glm(resolved ~ victim_age + victim_sex + victim_race, data = df, 
                                 family = binomial())),
    results = map(models, \(mod) broom::tidy(x = mod, conf.int = TRUE))
  ) |> 
  unnest(results) |> 
  rename(log_OR = estimate, log_conf_low = conf.low, log_conf_high = conf.high) |> 
  mutate(OR = exp(log_OR),
         conf_low = exp(log_conf_low),
         conf_high = exp(log_conf_high)) |> 
  select(city_state, term, OR, conf_low, conf_high) |> 
  filter(term == "victim_sexMale")

results |> 
  arrange(desc(OR)) |> 
  filter(min_rank(desc(OR)) < 11) |> 
  knitr::kable(digits = 3, caption = "Top 10 cities with highest adjusted OR for solving homicides comparing Male to Female victims")

```
In Albuquerque, the odds of solving a case in which the victim is male are 1.77 times higher than the odds of solving a case in which the victim is female. 95% CI (0.825, 3.76)


```{r, OR graph}
results |> 
  mutate(city_state = forcats::fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(
    aes(x = city_state,
        ymin = conf_low,
        ymax = conf_high)
  ) +
  labs(
    title = "Adjusted OR for solving homicides comparing Male to Female victims in each city",
    x = "city_state",
    y = "estimated OR"
  ) +
  theme(axis.text.x = element_text(angle=90, vjust=1, hjust=1))

```

In New York City, cases in which the victim is male have the lowest odds of being solved compared to cases in which the victim is female. In contrast, in Albuquerque, cases in which the victim is male have the highest odds of being solved compared to cases in which the victim is female.

# Problem 2
## Load Central Park Weather Data
```{r, load and clean weather data, message = FALSE}
cp_weather_df <-
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())

```
This dataset captures the daily weather recorded in Central Park during 2022. Variables include daily maximum and minimum temperature as well as total daily precipitation. The dataset has `r nrow(cp_weather_df)` rows and `r ncol(cp_weather_df)` columns. 

## Bootstrap

We generated 5000 bootstrap samples and fit our simple linear regression for tmax to each sample.

Note: `strap` (i.e. our bootstrap sample)

```{r, bootstrap, message = FALSE}
cp_results <- cp_weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results_1 = map(models, broom::tidy),
    results_2 = map(models, broom::glance)
    ) |> 
  unnest(results_1) |> 
  select(term, estimate, results_2) |> 
  unnest(results_2) |> 
  select(term, estimate, r.squared)
  
cp_results_tidy <- cp_results |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  mutate(log_coefficients_mult = log(tmin * prcp))

```
For each sample, we produced estimates for r.squared, the intercept, tmin (beta1), prcp (beta2), and log_coefficients_mult (log of beta1 * beta2).

## r.squared distribution
```{r, r squared distribution}
cp_results_tidy |> 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "R.squared distribution "
  ) 

```

The distribution of r.squared is slightly left skewed and centers around 0.92. This indicates that, on average, about 92% of the variation in tmax can be explained by the linear relationship between tmax and tmin + prcp.

## log(beta1 * beta2) distribution
```{r, log betas distribution}
cp_results_tidy |> 
  filter(log_coefficients_mult != "NaN") |> 
  ggplot(aes(x = log_coefficients_mult)) +
  geom_density() +
  labs(
    title = "log(beta1 * beta2) distribution "
  ) 

```

In calculating log_coefficients_mult (log of beta1 * beta2), `r cp_results_tidy |> filter(log_coefficients_mult == "NaN") |>  nrow()` NA values were produced. Since negative values for prcp are present and you cannot take the log of a negative value, NAs were produced.

NA values were filtered out in order to construct the distribution of log(beta1 * beta2). The distribution is left skewed and centers around -5.5

## Construct 95% Confidence Intervals
```{r, CIs}
cp_results_tidy |> 
  select(r.squared, log_coefficients_mult) |> 
  pivot_longer(
    r.squared:log_coefficients_mult,
    names_to = "term",
    values_to = "estimate"
  ) |> 
  group_by(term) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025, na.rm = TRUE),
    ci_upper = quantile(estimate, 0.975, na.rm = TRUE)) |>
  knitr::kable(digits = 2, caption = "95% Confidence Intervals")

```
We are 95% confident that the true value of log(beta1 * beta2) lies between -8.98 and -4.60.

We are 95% confident that the true value of r.squared lies between 0.89 and 0.94.

# Problem 3
## Load and clean birthweight data
```{r, load and clean birthweight data, message = FALSE}
birthweight_raw <- read_csv("data/birthweight.csv", col_names = TRUE)

birthweight_clean <- birthweight_raw |> 
  mutate(babysex = recode(
    babysex,
    "1" = "male",
    "2" = "female"),
    frace = recode(
      frace,
      "1" = "White",
      "2" = "Black",
      "3" = "Asian",
      "4" = "Puerto Rican",
      "8" = "Other",
      "9" = "Unknown"),
    mrace = recode(
      mrace,
      "1" = "White",
      "2" = "Black",
      "3" = "Asian",
      "4" = "Puerto Rican",
      "8" = "Other"),
    malform = recode(
      malform,
      "0" = "absent",
      "1" = "present")
    ) |> 
  mutate(
    babysex = forcats::fct_relevel(babysex, c("male", "female")),
    frace = forcats::fct_relevel(frace, c("White", "Black", "Asian", "Puerto Rican", "Other")),
    mrace = forcats::fct_relevel(mrace, c("White", "Black", "Asian", "Puerto Rican")),
    malform = forcats::fct_relevel(malform, c("absent", "present"))
  )

```

The birthweight dataset includes data about `r nrow(birthweight_clean)` children. The dataset has `r nrow(birthweight_clean)` rows and `r ncol(birthweight_clean)` columns. Examples of variables include sex of the baby, gestational age of the baby at birth, weight of the baby at birth, and monthly income of the family.

No actual values were recorded for Unknown within the `frace` variable, so Unknown was not included in the factor releveling. Similarly, no actual values were recorded for Other within the `mrace` variable, so Other was not included in the factor releveling. There were `r sum(is.na(birthweight_clean))` NA values present in our cleaned dataset.

## Construct a regression model for birthweight

Next, we created a model to understand which variables may influence a child's birthweight. Since birthweight is a continuous outcome, we started by constructing linear models using lm.

```{r, construct a model}
fit_1 = lm(bwt ~ gaweeks, data = birthweight_clean)
fit_2 = lm(bwt ~ gaweeks + bhead, data = birthweight_clean)
fit_3 = lm(bwt ~ gaweeks + bhead + blength, data = birthweight_clean)

fit_4 = lm(bwt ~ gaweeks + bhead + blength + babysex, data = birthweight_clean)
fit_5 = lm(bwt ~ gaweeks + bhead + blength + malform, data = birthweight_clean)

anova(fit_1, fit_2) |> 
  broom::tidy()

anova(fit_2, fit_3) |> 
  broom::tidy()

anova(fit_3, fit_4) |> 
  broom::tidy()

anova(fit_3, fit_5) |> 
  broom::tidy()
```
The first model employs only `gaweeks`. For the next model, we added `bhead` and ran an anova to assess the significance of the additional predictor. We repeated this process of adding a predictor and running an anova. The fifth model did not produce significant results. 

We decided to use the fourth model using the predictors `gaweeks` (gestational age), `bhead` (baby's head circumference at birth), `blength` (length of baby at birth), and `babysex` (baby's sex). When model 4 was compared to model 3, the anova produced a p-value of 0.000158, indicating `babysex` to be a significant additional predictor. Most of the selected variables are aspects related to the baby's size which could reasonably be suspected to contribute to a baby's weight. 

## Plot of model residuals against fitted values
```{r, resid graph for our model}
birthweight_clean |> 
  modelr::add_residuals(fit_4) |> 
  modelr::add_predictions(fit_4) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(
    title = "Model 4: Residuals against fitted values "
  ) 

```

Our proposed linear model of `lm(bwt ~ gaweeks + bhead + blength + babysex` appears to be an appropriate fit for these data. The points center around 0, indicating that the model is a good fit. In addition, the points are not arranged in a clear pattern, indicating a linear model to be suitable.

# Cross Validation

We compared our model to two other models in terms of the cross-validated prediction error.

First, we checked that the two alternative models were suitable options for the data by graphing the residuals against the fitted values. Both linear models were appropriate. 

```{r, check resid of alt models}
fit_alt = lm(bwt ~ blength + gaweeks, data = birthweight_clean)
fit_alt_int = lm(bwt ~ bhead * blength * babysex, data = birthweight_clean)

birthweight_clean |> 
  modelr::add_residuals(fit_alt) |> 
  modelr::add_predictions(fit_alt) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(
    title = "Fit_alt: Residuals against fitted values "
  ) 

birthweight_clean |> 
  modelr::add_residuals(fit_alt_int) |> 
  modelr::add_predictions(fit_alt_int) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  labs(
    title = "Fit_alt_int: Residuals against fitted values "
  ) 

```


```{r, graph rmse}
cv_df <-
  crossv_mc(birthweight_clean, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df <-
  cv_df |> 
  mutate(
    mod_alt1  = map(train, \(df) lm(bwt ~ gaweeks + bhead + blength + babysex, data = df)),
    mod_alt2  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod_alt3  = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df))) |> 
  mutate(
    rmse_alt1 = map2_dbl(mod_alt1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_alt2 = map2_dbl(mod_alt2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_alt3 = map2_dbl(mod_alt3, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(rmse_alt1, rmse_alt2, rmse_alt3) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(
    title = "Distribution of rmse for each model "
  ) 

```

The distribution of the cross-prediction error (rmse) is similar for the first and third model.

```{r, avg rmse}
cv_df |> 
  select(rmse_alt1, rmse_alt2, rmse_alt3) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  group_by(model) |> 
  summarize(mean_rmse = mean(rmse)) |> 
  knitr::kable(digits = 2, caption = "Mean rmse for each model")

```

When calculating the mean rmse for each model, the mean rmse for the first model is slightly lower than that of the third model. Thus, we recommend using the first model, which is our proposed model: `lm(bwt ~ gaweeks + bhead + blength + babysex`.









